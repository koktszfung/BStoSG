import torch
import torch.nn
import torch.nn.functional
import torch.optim.adam
from torch.utils.data import Dataset, DataLoader
import os
import json
import numpy as np


class Set(Dataset):

    # TODO: create a meaningful variable name for search_start, search_end
    def __init__(self, data_dir, search_start, search_end, num_data):  # length: number of data
        self.len = num_data
        self.data_input = []
        self.data_label = []
        for subdir, dirs, files in os.walk(data_dir):  # search through the directory
            search_size = abs(search_end - search_start)  # separate input_data into test set and train set
            order = np.random.permutation(search_size) - 1 + search_start  # randomly add to set without repetition
            for i in range(search_size):  # iterate through order
                file_name = files[order[i]]
                with open(data_dir + file_name) as file:
                    data_json = json.load(file)
                    data_input_np = np.array(data_json["bands"])  # load bands into nd-array
                    if data_input_np.shape[0] != 30:  # accept only data with 30 energy bands
                        continue
                    data_input_np = data_input_np.flatten().T
                    data_label_np = np.zeros(230).T
                    data_label_np[data_json["number"] - 1] = 1
                self.data_input.append(torch.from_numpy(data_input_np).float())
                self.data_label.append(torch.from_numpy(data_label_np).float())
                if len(self.data_input) >= num_data:
                    break

    def __len__(self):
        return self.len

    def __getitem__(self, index):
        return self.data_input[index], self.data_label[index]


class Net(torch.nn.Module):
    """
    default function: __init__, forward, backward (generated by autograd)
    """

    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = torch.nn.Linear(360, 100)
        self.fc2 = torch.nn.Linear(100, 100)
        self.fc3 = torch.nn.Linear(100, 230)

    def forward(self, x):
        # relu: activation, self.fc(x): pass previous output as input through fc
        x = torch.nn.functional.relu(self.fc1(x))
        x = torch.nn.functional.relu(self.fc2(x))
        x = self.fc3(x)  # don't relu output
        return x


def train(device: torch.device, network: torch.nn.Module, optimizer, criterion,
          data_loader: DataLoader) -> torch.tensor:
    print("train start")
    for batch in data_loader:
        batch_input, batch_label = batch
        batch_size = batch_input.shape[0]
        for i in range(batch_size):
            # reset gradient history
            optimizer.zero_grad()  # zero the gradient buffers
            # read data
            data_input, data_label = batch_input[i], batch_label[i]
            data_input, data_label = data_input.to(device), data_label.to(device)
            # calculate loss
            output = network(data_input)
            loss = criterion(output, data_label)
            # optimize
            loss.backward()  # backpropagate and store changes needed
            optimizer.step()  # update weight and bias
            # progress bar
            if i % int(batch_size / 10) == 0:
                print("\r{}%".format(i / batch_size * 100), end="")
    print("\r100%")
    print("train end")
    return network.named_parameters()


def test(device: torch.device, network: torch.nn.Module, criterion,
         data_loader: DataLoader) -> torch.tensor:
    print("test start")
    loss = 0
    for batch in data_loader:
        batch_input, batch_label = batch
        batch_size = batch_input.shape[0]
        for i in range(batch_size):
            with torch.no_grad():
                # read data
                data_input, data_label = batch_input[i], batch_label[i]
                data_input, data_label = data_input.to(device), data_label.to(device)
                # calculate loss
                output = network(data_input)
                loss += criterion(output, data_label).item()/batch_size
            # progress bar
            if i % int(batch_size / 10) == 0:
                print("\r{}%".format(i / batch_size * 100), end="")
    print("\r100%")
    print("test end")
    return loss


def main():
    # setup
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    net = Net().to(device)

    optimizer = torch.optim.Adam(net.parameters(), lr=0.01)
    criterion = torch.nn.MSELoss()

    train_loader = DataLoader(dataset=Set("input_data/", 0, 5000, 1000), batch_size=1000, shuffle=True)
    train(device, net, optimizer, criterion, train_loader)
    test_loader = DataLoader(dataset=Set("input_data/", 5000, 10000, 1000), batch_size=1000, shuffle=True)
    loss = test(device, net, criterion, test_loader)
    print(loss)


if __name__ == "__main__":
    main()
    torch.cuda.empty_cache()
